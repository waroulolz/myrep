\documentclass[11pt,a4paper,oneside]{book}
\usepackage[hmargin={1.25in,1.25in},vmargin={1.25in,1.25in}]{geometry}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage[inline]{enumitem}
\usepackage{xcolor}
\usepackage{tikz}
\linespread{1.5}
\lstset{ %
  language=R,                     % the language of the code
  basicstyle=\footnotesize,       % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},      % keyword style
  otherkeywords={},
  commentstyle=\color{green},     % comment style
  stringstyle=\color{purple},     % string literal style
  escapeinside={\%*}{*)},         % if you want to add a comment within your code
  morekeywords={*, svmForecast, ly, getTestDf, renderPlotly, output, predPlot},            % if you want to add more keywords to the set
  deletekeywords={df, date, mode, window, gamma, rownames}
} 


\newenvironment{myitemize}
{ \begin{itemize}
    \setlength\itemsep{0pt}}
{ \end{itemize}                  } 


\makeatletter
\newcommand{\inlineitem}[1][]{%
\ifnum\enit@type=\tw@
    {\descriptionlabel{#1}}
  \hspace{\labelsep}
\else
  \ifnum\enit@type=\z@
       \refstepcounter{\@listctr}\fi
    \&\ \@itemlabel\ 
\fi}
\makeatother

\begin{document}
\nocite{*}

\begin{titlepage}
\begin{center}
\textbf{\textsc{UNIVERSIT\'E LIBRE DE BRUXELLES}}\\
\textbf{\textsc{Faculté des Sciences}}\\
\textbf{\textsc{Département d'Informatique}}
\vfill{}\vfill{}

\begin{center}{\Huge Machine learning approach for multiple financial time series forecasting}\end{center}{\Huge \par}
\begin{center}{\large Jérôme Bastogne}\end{center}{\Huge \par}
\vfill{}\vfill{}
\includegraphics[keepaspectratio=true,scale=0.9]{img/ulbBlack.pdf}
\vfill{}\vfill{}
\begin{flushright}{\large \textbf{Promoteur :}}\hfill{}{\large Mémoire présenté en vue de}\\
{\large Prof. Gianluca Bontempi}\hfill{}{\large l'obtention du grade de}\\
{\large Ir. Jacopo De Stefani}\hfill{}{\large Licencié en Informatique}\end{flushright}{\large\par}
\enlargethispage{1cm}
\textbf{Année académique 2016~-~2017}
\end{center}
\end{titlepage}


\chapter*{Acknowledgements}


\tableofcontents


\chapter*{Abstract}


\chapter{Introduction}


Financial time series analysis drew significant attention in recent years. Traders, investors, markets analysts, even anyone who follows the stock markets are all interested in a better understanding of price fluctuations. With all the medias and high-tech information constantly available, the stock prices can radically change from a day to another. 
The risks associated to falling prices are real and traders have to cope with them daily. A better understanding of stock prices behaviour would help traders to make decisions. This explains the consequent interest increase in accurate forecasting of future fluctuations.

The forecasting domain has largely been covered by linear statistical methods such as : ARIMA, GARCH... However, financial time series are often nonlinear and those methods are not able
to capture the non-linearity of the financial time series. Some nonlinear methods have been studied, but they haven't been analysed as deeply as the linear ones described before. Furthermore, generally in order to work, statistical methods requires that the data meet some assumptions, such as requiring stationary series,... On the other hand, machine learning is able to find patterns in data even without knowing the structure of it. 

Machine learning might therefore be more adequate to the forecasting of those series. Machine learning is booming and yet has proven to be high performing. Also, the huge increase of the available data makes it too large for traditional analytics. Therefore machine learning is a more suitable choice.

The aim of my work is twofold. On one hand I will focus on using machine learning techniques to forecast the future of financial time series. In particular, my goal is to compare the support vector machine algorithm's performance to more established algorithms for financial time series.

On the other hand, the second objective is the development of an autonomous analysis tool to perform and compare multiple machine learning algorithms on financial times series. All the techniques described in the methodology section will be put into a tool that can be used without any prior knowledge about machine learning.

One will find in this document an overview of the state of art concerning financial time series in chapter \ref{backgr}. First there will a quick reminder of fundamentals and basic definitions. After that, chapter \ref{methodo} will be dedicated to the methodology and the procedures to follow in order to achieve machine learning forecasting. Next will follow a presentation of the developed tool in chapter \ref{tool}. Finally, chapter \ref{results} will be devoted to the results of experiences measuring the machine learning algorithm's performances.




\chapter{Background} \label{backgr}

In this chapter, I'll review the basics of the prediction of time series in order to have a brief introduction to machine learning. Therefore this chapter will be a quick reminder of fundamentals alongside some formal basic definitions. 



\section{Efficient Market Hypothesis}

The efficient-market hypothesis (EMH) states that stock prices fully reflect all available information and that prices are immediately corrected by any news. This would mean that the market is unbeatable and therefore, forecasting would be impossible as the main assumption of forecasting is that any information of the past has some incidence on the future. Also some evidence argue against EMH proving that stock market is not efficient. Indeed it wrongly assumes that all investors are always rational. Also, sometimes only a few traders have knowledge of a particular new information jeopardising this theory. However, if this theory would hold, the best possible forecast would only be the addition of noise \cite{aamodt} : 

\begin{center}
    $Y_{n+1} = Y_{n} + \epsilon_{n}$
\end{center}


The question if the market is truly unbeatable still didn't reach any consensus. So while it is still only a theory, it is important to have an overview of the subject. \cite{emh}


\section{Definitions}


\subsection{Machine learning}

A very well known quote from \textit{Tom Mitchell} defines machine learning as : 

\vspace*{\fill} 
\begin{quote} 
\centering 
``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E'' -- \textit{Tom Mitchell, Carnegie Mellon University}. 
\end{quote}
\vspace*{\fill}


Machine learning is helpful when there is a need of understanding a time series behaviour to make predictions on it, even if nothing is known about the nature of the time series. There exist different types of learning algorithms and some of them will be discussed in section \ref{MLalgs}.


\subsection{Time series}

A time series is a finite sequence of $n$ data points $Y_{t}$ $\epsilon$ $\Re$ observed over time. Points are often equally spaced by time intervals. All points are ordered by their timestamp and changing that order would change the meaning of the data. \cite{hyndman}

\subsubsection{Examples}

\begin{myitemize}
    \item Numbers of sales of a company each month of year 2016.
    \item Temperature measurements in a country over 20 years.
    \item Electricity consumption in Belgium measured every half-hour for 100 days, representing 144000 values.
    \item Unemployment rate in the United States (figure \ref{fig:US}).
\end{myitemize}

\begin{figure}[ht]
  \centering
    \includegraphics[scale=0.5]{img/Unemployment-Rate.png}
  \caption{Unemployment rate in the United States}
  \label{fig:US}
\end{figure}



\subsection{Forecasting}

One of the biggest interests of studying time series is forecasting. Indeed, based on the previous observations of a time series $Y_{1},...,Y_{n}$, could we predict the future observations  $Y_{n+1}, Y_{n+2},...$? 

It is possible under the strong assumption that a dependency between the past and the future exists. The challenge is to find the \textit{\textbf{model}} that describes the best the pattern of the time series.

Forecasts can be required for short-term, medium-term and long-term purposes. Short-term forecasting is about close in time scheduling and decision making, such as scheduling of tasks, personnel,... Medium-term forecasting helps to determine future resource requirements, such as hiring personnel, acquiring needed knowledge to handle particular tasks,... Finally, long-term forecasting is used for strategic planning with a long-term horizon. It's often a help to make long-term business decisions for a company. \cite{hyndman}





\section{Stochastic models}

Choosing a model is an important step in time series forecasting. A model's principal function is to describe how the most important \textit{\textbf{features}} of a time series can be combined to form a meaningful prediction. A \textit{\textbf{feature}} is an individual measurable characteristic, a variable. A model should explain how the past influenced the present and could be used to extrapolate the future. And if handling multiple time series, a model should explain how they affect each other. Finally, a model should accurately forecast future values of a time series. \cite{hyndman}

We will first have a look at traditional stochastic models, such as ARMA and ARCH, that were studied for a very long time. Later, we will compare them with some machine learning algorithms that, in comparison, are still booming.


\subsection{ARMA}

ARMA is the main statistical model for univariate linear time series forecasting. ARMA is a combination of auto regressive (AR[p]) and moving average (MA[q]) models. Auto regressive means that each value from the time series is regressed based on its previous values. Moving average means that the model uses previous forecast errors in a regression-like model. ARMA's general form is : 

\begin{equation}
Y_{t} = \sum\limits_{i=1}^p \phi_{i} Y_{t-i} +  \sum\limits_{i=1}^q \theta_{i} \epsilon_{t-i} + \epsilon_{t} 
\end{equation}

where $\phi_{i}$ and $\theta_{i}$ are the constant parameters and $\epsilon_{t}$ are independent errors also called \textit{\textbf{white noise}}. White noise is a sequence of components with a zero mean, a finite variance and that are uncorrelated.

However, the problem with ARMA is that it requires the time series to be stationary which isn't always the case. A stationary process is a process from which the mean, the variance and it's covariance are constant over time. Otherwise, when the variance isn't constant, an ARCH model is the better choice. \cite{Holan}


\subsection{ARCH \& GARCH}

ARCH in an auto regressive (AR) conditional heteroscedastic (CH) model. Heteroscedastic means that the variance of a variable isn't constant over a determined amount time. It's useful when studying financial time series because it illustrates the evolution of high variance changes for short periods of time, also described as volatility clustering. For example, if a market incur a substantial drop, it can alert people to sell their stuff before it drops even more. Therefore we can observe conditional periods of increased variance followed by calmer periods. \cite{Holan}

So, if we use ARCH as an auto regressive model of the variance of the series $y_t$ with zero mean, we can write the ARCH(q) model as : 
\begin{equation}\label{eq:arch}
\begin{matrix}
y_t = \sigma_t \epsilon_t \\
with \ \epsilon_t \sim iid(0,1) \\
\sigma_t^2 = Var(y_t \mid y_{t-1},..., y_{t-q}) = \alpha_0 + \alpha_1 y_{t-1}^2 + ... + \alpha_q y_{t-q}^2
\end{matrix}
\end{equation}


From there, GARCH (generalised ARCH) was introduced by adding moving-average components of the conditional variance to the ARCH model. The GARCH(p,q) model changes the definition of $\sigma_t^2$ in equation \ref{eq:arch} as : 

\begin{equation}
\sigma_t^2 = \alpha_0 + \sum_{j=1}^p\beta_j \sigma_{t-j}^2 + \sum_{k=1}^q\alpha_k y_{t-k}^2 
\end{equation}

where $ \beta_1,..., \beta_p \geq 0$ and $\alpha_1,...\alpha_q \geq 0$. \cite{Holan}



\section{Volatility}

Volatility defines the amplitude of variations of a time series. It is a key feature for financial time series analysis. Volatility represents a risk measure which is therefore directly related to risk management, a primary task of investors. Financial individuals are vulnerable to time changing volatility that causes uncertainty in risk assessment. 

Volatility is a vague notion and an unobservable variable. It can be interpreted in multiple ways and therefore it has to be calculated with a proxy of conditional variance. It can either be seen as the standard deviation computed over a moving window of a determined size, it can be computed as the variance between returns, or it can be one of some of the definitions of volatility given by Garman and Klass \cite{garm} which will be described later in section \ref{vol}.

While GARCH models are known to be very well suited for volatility forecasting as we have seen, I'll try to show through this thesis the capabilities of machine learning in that domain.

Note that volatility forecasting is directly linked with multi-step-ahead forecasting here. Indeed, knowing the risk degree only one day ahead is not enough for traders to react and take actions on the market. Having an idea of the behaviour of volatility several days in advance has a great added value for investors.




\chapter{Methodology}\label{methodo}

This chapter is dedicated to the \textit{modus operandi} of machine learning and forecasting. One can have an overall overview of the machine learning procedure on figure \ref{fig:ML}; the main \textit{\textbf{procedures}} are represented on the figure by black rectangles, and the blue ones represent what we have before the corresponding procedures, and what we get after each procedure. We will first have a look at the pre-processing phases. Then we will see the theory behind the models used in the tool. Finally, we will see how to choose a strategy with a model in order to make good predictions and how to quantify their performance.

\begin{figure}[ht]
  \centering
    \includegraphics[scale=0.55]{img/learningprocedure.png}
  \caption{Time series learning procedure to forecasting.}
  \label{fig:ML}
\end{figure}



\section{Pre-processing}

\subsection{Structure of the data}

One can choose to use the tool to forecast the original data as is, or can choose to forecast the continuously compounded returns or the volatility of the data instead.


\subsubsection{Prices (OHLC)}\label{OHLC}

When handling financial time series, a common dataset structure is a composition of : opening, high, low and closing prices (OHLC) and the volumes of the stock for each weekday. The OHLC format is very often used to view stock movements and provides more information for volatility calculations as explained in section \ref{volatility}. The low and high prices are the lowest and highest prices indices for a day and the opening and closing are respectively, the price at the start of the day when the stock market opens and the price when the stock market closes.


\subsubsection{Log returns}

Log returns are often more interesting for financial time series because they show an unscaled value comparable with other returns. It is defined as :

\begin{equation}
r_{t_i} = \ln\frac{P_{t_i}}{P_{t_{i-1}}}
\end{equation}

where $P_{t_i}$ defines the price at time $i$. The last price when the stock market closes is the better choice for this type of calculations.


\subsubsection{Volatility}\label{vol}

On the other side, volatility is the magnitude of changes in the prices of a financial asset. There are multiple ways of calculating the volatility of a financial time series, and one of the most suited should be one that considers historical OHLC prices (see \ref{OHLC}). The estimator of volatility used in this tool is one of the most practical and efficient based on a study from Garman and Klass \cite{garm}:

\begin{equation}\label{volatility}
\begin{matrix}
u = \ln\frac{P_t^{(h)}}{P_t^{(o)}} \qquad
d = \ln\frac{P_t^{(l)}}{P_t^{(o)}} \qquad
c = \ln\frac{P_t^{(c)}}{P_t^{(o)}} \\
\hat{\sigma}\left ( t \right ) = 0.511\left ( u - d \right )^2 - 0.019\left [ c\left (u + d\right ) - 2ud\right ] - 0.383c^2
\end{matrix}
\end{equation}

where $P_t^{(h)}$, $P_t^{(l)}$ and $P_t^{(c)}$ are respectively the high, low and opening prices at time $t$.


\subsection{Normalisation}

Normalisation is often required by some algorithms to ensure convergence and it is also a stable way of comparing multiple models and algorithms. More importantly, it prevents a feature from dominating the others by being much larger. Therefore, before using any machine learning algorithm, the data is normalised as follows : 

\begin{equation}
{x}' = \frac{x - \mu }{\sigma }
\end{equation}

so that we have a normal dataset. Note that $\mu$, the mean of x, and $\sigma$, its standard deviation, both are calculated on the training set only with which the model will be trained. In this way, we avoid possible biases induced by not supposedly known information.




\section{Models}\label{MLalgs}

\subsection{Support vector machines}

\subsubsection{Definition}

Support vector machines (SVM) is a supervised learning model that performs classification by constructing an multi-dimensional hyperplane that optimally separates the data into two categories. 

SVM implements the structural risk minimisation principle which seeks to minimise an upper bound of the generalisation error rather than minimise the training error.

SVM uses linear models to implement nonlinear class boundaries through some nonlinear mapping of the input vectors into a higher dimensional feature space. A linear model constructed in the new space can represent a nonlinear decision boundary in the original space as seen on figure \ref{fig:svm}. In the new space, an optimal separating hyperplane is constructed. \cite{kim}\cite{liwang}\cite{Smola}


\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.45]{img/svm.png}
  \caption{Transformation of the input space into a higher dimensional space with a kernel $\phi$ where the non-linearly separable data can be separated linearly by a hyperplane.}
  \label{fig:svm}
\end{figure}


The support vectors are defined by the training examples that are closest to the maximum margin hyperplane and the support vector's number of coordinates are defined by the dimension space size. The maximum margin hyperplane gives the maximum separation between the decision classes as shown on figure \ref{fig:svm_linear_sep}. 

\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.3]{img/svm1.png}
  \caption{SVM linearly separable case.}
  \label{fig:svm_linear_sep}
\end{figure}

Support vector machines can be applied to financial times series as a case of regression (SVR) by performing a linear regression in high dimensional feature spaces (instead of separating classes). They define the \textit{\textbf{loss function}} that ignores errors which are situated within the certain distance of the true value.

The transformation of a space into a higher dimensional space is done via a \textit{\textbf{Kernel}}. Kernel functions have some tuning parameters. Overfitting can be avoided by allowing a small portion of the training data to lie outside the margin thanks to the slack parameter. For financial time series forecasting, a combination of kernels may be used as information can behave from different sources. This introduces the importance of choosing good parameters and the good combination of kernels (kernel selection). \cite{kim}\cite{liwang}\cite{Smola}


\subsubsection{Support vector regression theory}

Support vector machines will be detailed more in depth as it is the most novel algorithm and also the most promising. Therefore, this subsection will describe the most important mathematical steps to understand the intrinsic working of SVMs.

Given a training set $\left \{ \left ( x_{1}, y_{1} \right ), \left ( x_{2}, y_{2} \right ), ... \left ( x_{l}, y_{l} \right ) \right \}$ where $y_{i}$ is the output for the vector input $x_{i}$ and $x_{i} \in R^{n} , y_{i} \in R,  \forall i \in \left \{ 1,2, ...,l \right \}$, our goal is to find a function $f\left ( x \right )$ as flat as possible to make predictions different from at most $\varepsilon$ from those $y_{i}$ values for all the training data. This is called $\varepsilon$-SVM regression. $\varepsilon$-SVM makes that small deviations in the input still leave to the same output, making the method more robust. The $\varepsilon$-sensitive loss function that allows errors inside the $\varepsilon$ margin to be considered as zero is then given by :

\begin{equation}
L_{\varepsilon} = \left\{\begin{matrix}
0 \qquad\qquad\ \   if \ \left | y - f\left ( x \right )\right | \leq \varepsilon \\ 
\left | y - f\left ( x \right )\right | - \varepsilon \qquad otherwise
\end{matrix}\right.
\end{equation} \cite{Cortes}\cite{Smola} 

The empirical risk (training error) is given by the means of the errors :

\begin{equation}
R_{emp} = \frac{1}{n} \sum_{i=1}^n L_{\varepsilon}\left ( y_{i}, f\left ( x_{i} \right ) \right )
\end{equation}

Let's see how it works with the linear case where our function $f$ has the form :

\begin{equation}
f\left ( x \right ) = \left \langle w, x \right \rangle + b  \text{ with } w \in R^{n}, b \in R
\end{equation}

where $\left \langle ., . \right \rangle$ is the inner product.

To find a large-margin classifier, we want to minimise the norm of $w$. Therefore, by using regularisation our solution to the problem is :

\begin{equation}
\begin{matrix}
\min \frac{1}{2} \left \| w \right \|^{2}\\
\text{constrains:}
 \left |  y_{i} - (\left \langle w, x_{i} \right \rangle + b) \right | \leq \varepsilon 
 \end{matrix}
\end{equation}


\begin{figure}[!h]
  \centering
    \includegraphics[scale=1]{img/SVR_2.png}
  \caption{Soft margin with the $\varepsilon$ bands and the $\xi$ slack variables. \cite{Sayad}}
  \label{fig:svr}
\end{figure}

We have to bear in mind the case where no such function $f\left ( x \right )$ satisfying the constraints for all points exists. So we have a problem when the optimisation problem is infeasible. Therefore, similarly to the "soft margin" loss function, we add positive slack variables $\xi_{i}\xi_{i}^{*}$ to allow some regressions errors (see figure \ref{fig:svr}). The principle is to separate the training set with a minimal number of errors. The addition of slack variables gives us a new formulation of the solution as :

\begin{equation}
\begin{matrix}
\min \frac{1}{2} \left \| w \right \|^{2} + C\sum_{i=1}^{l}(\xi_{i}+\xi_{i}^*)\\
\text{constrains :}
\left\{\begin{matrix}
y_{i} - (\left \langle w, x_{i} \right \rangle + b)\leq \varepsilon + \xi_{i}\\ 
-(y_{i} - (\left \langle w, x_{i} \right \rangle + b))\leq \varepsilon + \xi_{i}^*\\ 
\xi_{i} \geq 0\\ 
\xi_{i}^*\geq 0
\end{matrix}\right. 
\end{matrix}
\end{equation}


$C$\label{cost} is the constant parameter that represents the trade-off between the complexity of the model (i.e. the flatness of $f\left ( x \right )$, regularisation) and the number of regression errors (i.e. values outside the $\varepsilon$ margin, empirical risk). $C$ is called the regularised constant and takes care of avoiding overfitting. The higher $C$ is, the more we approach a hard-margin function and the less we will have misclassified individuals (overfitting).


By introducing Lagrange multipliers $\alpha_i \geq 0$, $\alpha_i^*\geq 0$ for each observation $x_i$, we can transform  $f\left ( x \right )$ and access it's dual form which is easier to solve, but also it will help us for the nonlinear cases \cite{Cortes}\cite{Smola} : 


\begin{equation}
\begin{matrix}
\max -\frac{1}{2} \sum_{i,j = 1}^l \left ( \alpha _i - \alpha _i^* \right )\left \langle x_i, x_j \right \rangle - \varepsilon\sum_{i=1}^l\left ( \alpha _i + \alpha _i^* \right ) + \sum_{i=1}^l y_i\left ( \alpha _i - \alpha _i^* \right )\\
\text{constrains :}
\sum_{i=1}^l \left ( \alpha _i - \alpha _i^* \right) = 0 \\
0 \leq \alpha _i \leq C \\
0 \leq \alpha _i^* \leq C
\end{matrix}
\end{equation}


This equation can be solved by Sequential Minimal Optimisation and the solution to the regression problem is finally given by \cite{Cortes}\cite{Smola}:

\begin{equation}
f\left ( x \right ) = \sum_{i=1}^l \left ( \alpha_i - \alpha_i^* \right )\left \langle x_i, x \right \rangle + b
\end{equation}



For the nonlinear case, we don't search for the flattest function in the input space but rather in the feature space. The usefulness of the kernel comes from the fact that we don't have to explicitly compute the mapping of the input vector into the feature space $\phi(x)$. Instead we compute the inner products between the images of all pairs of data in the feature space very efficiently, this is known as the "kernel trick". The solution to the regression becomes :


\begin{equation}
f\left ( x \right ) = \sum_{i=1}^l \left ( \alpha_i - \alpha_i^* \right )K\left ( x_i, x \right ) + b
\end{equation}

where $K\left ( x_i, x \right )$ is the kernel function. The most popular kernel is named the Gaussian radial basis function kernel. The Gaussian RBF makes it the default kernel to use with no prior knowledge on the data and it is defined as : 

\begin{equation}
K\left ( x_i, x \right ) = \exp \left ( -\gamma \left \| x - x_i \right \|^2 \right )
\end{equation}

where $\gamma$\label{gamma} is the parameter that defines how far the influence a single training example is. It might be considered as a regularisation parameter. With a high gamma, the closest training examples from the decision boundary pull a lot more weight than farthest ones. On the other side, a small gamma means that the influence of $x_i$ is more important (and so imply large variance, small bias) and vice versa.

As a final note, one should keep in mind that he can combine multiple kernels in order to better capture the feature's natures, especially if the input is made of different sources.


\subsection{K-Nearest Neighbors}

KNN is a local nonlinear classification model that can also be used for regression. KNNs are often referred to as one of the simplest of all machine learning algorithms. Its principle is to find in the training set the input that is the most similar because similar input would have similar outputs. Note that here the input is a vector of points, and the size of the vector is the order of the model. 

Similarity between inputs can be computed with any distance function such as the Euclidean distance ($\sqrt{\sum_i \left (  x_i - y_i \right )^{2}}$). The kNN model output is then given by the mean of the k-nearest neighbors output as follows :


\begin{equation}
f\left ( x \right ) = \frac{1}{k}\sum_{x' \in kNN(x) } f\left ( x' \right )
\end{equation}

where $kNN(x)$ is the set of the k-nearest neighbors of $x$. There also exists a softer version of the k-nearest neighbors algorithm where instead of computing a mean, we compute a weighted average of each neighbor proportionally to their distance, the closer, the higher the weight is. The parameter k defines the bias-variance trade-off of the algorithm. The higher k is, the more samples we take from our training set, the lower the variance and the higher the bias are. In the other case, a small k will lead to overfitting. \cite{navot}



\subsection{Naive model}

Naive models can be computed in multiple ways. They generally are very simple and try to forecast the future as a mean of the past or a repetition of the last known value. In my case, I decided to implement the naive model by computing and repeating for each horizon, the mean of the last thirty values, i.e.\ a month more or less. So my naive model is defined as : 

\begin{equation}
\hat{y}_t = \frac{1}{k} \sum_{i=0}^{k} y_{t-1-i}
\end{equation}

where k I fixed to the number of days in a month.


\section{Forecasting strategies}\label{strat}

Once we have a prediction model, we need a strategy in order to compute the actual predictions. If the model is our ``regression function'', then the strategy is how to apply it to the time series for forecasting. Predictions can be done in two ways :

\begin{myitemize}
    \item One-step-ahead prediction.
    \item Multi-step-ahead prediction.
\end{myitemize}

Supervised learning approaches cover with success one-step-ahead prediction, which is the ability to predict the very next unknown value. However, multi-step-ahead prediction remains quite challenging.  k-step-ahead prediction is the prediction of the next $k$ unknown values of the time series. 

\subsection{Multi-step-ahead strategies}

There exists some strategies for k-step-ahead prediction in the literature which the most common are \cite{Bonte}\cite{taiebonte}:

\begin{itemize}
    \item \textbf{Recursive approaches} : (also called iterated approaches) we iterate $k$ times with a one-step-ahead predictor. Each time a value $\phi_{t+1}$ is predicted, it is used as input for the next iteration and therefore, errors propagate consequently. Recursive strategies can therefore be biased.
    \item \textbf{Direct approaches} : we make $k$ direct separate predictions $\phi_{t+h}$ $\vee h \in \left[1, k\right]$ for the next $k$ unknown values, a different forecasting model being used for each horizon. Though, direct approaches makes a conditional independence assumption which may be incorrect to do. It also needs bigger time series than the iterated approach because of iterated generative's nature. Direct strategies have then less inputs in comparison. This leads to high variance for little time series or large horizon forecasting.
    \item \textbf{MIMO (multi-input multi-output) approaches} : The multi-input multi-output regression model returns a vector of $k$ consecutive predictions $\left\{\phi_{t+1},...,\phi_{t+k}\right\}$ in one step. MIMO mimics the direct approach strategy but preserving the stochastic dependency of the time series. The problem involved with MIMO is that in order to keep that stochastic dependency, all horizons have to be forecasted with the same model structure which can be restrictive. \cite{Bonte}\cite{taiebonte}
    \item \textbf{Rectify approach} : Rectify is a strategy that takes advantage of the strengths of both recursive and direct strategies. The principle is to first produce the predictions with the recursive strategy with a linear base model. As aforementioned, the results will be biased. Therefore, the second step of the strategy is to rectify the errors by adjusting the predictions with a nonlinear model with the direct strategy. This is supposed to reduce the bias of recursive linear systems. Rectify also proves to decrease variance's forecasts. \cite{BenTaieb}
\end{itemize}


\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.5]{img/bias-variance.png}
  \caption{Fundamental bias-variance trade-off of forecasting strategies.}
  \label{fig:biasvariance}
\end{figure}



\subsubsection{Dataset embeddings for multi-step forecasting}

The strategies described in this section above require the data to be held into a certain data structure. The structures will be described for the strategies used in the tool: the recursive and direct ones. 

Indeed, we want the model to learn how to predict a value $Y$ at time $t$ according to a number past values, so we need to train a model that is able to predict $Y_t$ according to its $k$ past values. The number of $k$ past values, called the order of the model, is fixed beforehand.  

E.g.\ , for the recursive approach, if the order of the model is 4, our model will try to predict $Y_t$ according to  $Y_{t-1}, Y_{t-2}, Y_{t-3}, Y_{t-4}$. Therefore our model is trained by passing a structure as shown on figure \ref{fig:recursive} where each input is a vector of $k$ past values of each input $Y_t$ in the training set, where $k$ is the order of the model, and where the corresponding output is the value $Y_t$. 
 

\begin{figure}[!h]
\centering
\begin{minipage}{\textwidth}
\begin{minipage}{0.5\textwidth}
\begin{center}
\textbf{Recursive}
\begin{itemize}
\item A single model for every horizon $h$.
\item Forecast at step $h$ based on forecast at step $h-1$.
\end{itemize}
\vskip10pt
   \begin{footnotesize}
   \begin{tabular}{|cccc|c|}
   \hline
   \multicolumn{4}{|c|}{\textbf{input vector}} & \textbf{y} \\ \hline
   $y_{1}$ & $y_{2}$ & $y_{3}$ & $y_{4}$ & $y_{5}$\\ \hline
   $y_{2}$ & $y_{3}$ & $y_{4}$ & $y_{5}$ & $y_{6}$\\ \hline
   $...$ & $...$ & $...$ & $...$ & $...$\\ \hline
   $y_{t-4}$ & $y_{t-3}$ & $y_{t-2}$ & $y_{t-1}$ & $y_{t}$ \\ \hline
   \end{tabular}
   \end{footnotesize}
\end{center}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{center}
\textbf{Direct}
\begin{itemize}
\item A single model for each horizon $h$.
\item Forecast at $h$ step is made using $h^{\text{th}}$ model.
\end{itemize}
\vskip12pt
   \begin{footnotesize}
   \begin{tabular}{|cccc|c|}
   \hline
   \multicolumn{4}{|c|}{\textbf{input vector}} & \textbf{y} \\ \hline
   $y_{1}$ & $y_{2}$ & $y_{3}$ & $y_{4}$ & $y_{6}$ \\ \hline
   $y_{2}$ & $y_{3}$ & $y_{4}$ & $y_{5}$ & $y_{7}$ \\ \hline
   $...$ & $...$ & $...$ & $...$ & $...$ \\ \hline
   $y_{t-7}$ & $y_{t-6}$ & $y_{t-5}$ & $y_{t-4}$ & $y_{t-2}$ \\ \hline
   \end{tabular}
   \end{footnotesize}
\end{center}
\end{minipage}
\end{minipage}
\caption{Data structures required for the recursive and direct approaches.}
\label{fig:recursive}
\end{figure}


Therefore one additional step of the pre-processing is to embed the data into a similar structure than can be passed to a model according to the strategy used. Note that for the direct strategy, as explained, one model has to be built for each horizon $h$ and therefore, each model requires the data to be re structured into a similar matrix but with different offsets of $h$.



\section{Learning procedure}

Now that we have covered the theory on models and strategies, we can have a look at the different phases of the learning procedure as shown on figure \ref{fig:ML}.


\subsection{Model generation}

We want to find a function that gives us the closest values to the output depending on the input, output being the forecasted values that we want to fit to the test data and the input being our training data. That function is often called the hypothesis. \cite{BenTaieb}


\subsection{Parametric identification}

The parametric identification selects the parameters that minimises the disparity between the forecasted values and the output. Note that not all models have parametric identification. In the case of the SVM, we search for the values of the cost and gamma parameters that reduce the most the gap between the forecasted values and the real output. For KNN, we have to find the number $k$ of instances that we take into account to determine similarities with classes. \cite{BenTaieb}


\subsection{Model validation}

The model validation phase is the process of examining the accuracy of the model. Estimating the errors is important, and if the evaluation is not satisfying enough, one should take the learning process back to model generation. A good way of measuring the performance of a model is by proceeding with a rolling window. Indeed, I chose a validation with a rolling window instead of a classical cross-validation because it ignores the temporal dependencies of time series. \cite{BenTaieb}


\subsubsection{Rolling window forecasts}\label{rwindow}


Rather than checking the performance of a model at one point in time, we would like to have a idea of how the model will generally perform. Sometimes we will have very accurate results, and other times the results will be worse, but we would like to have an idea of its general performance.

The principle is to use the model through the whole time series with a rolling window of a fixed size as shown on figure \ref{fig:rolling}. For each window, the data is separated into a training and a testing set. The model is then trained and predictions are made. By comparing those with the actual values in the testing set, we can compute errors and by averaging those over the whole time series. In the end, we get an average of performance indicator.

In the web tool I designed, the minimum size of the window is taken as two-thirds of the dataset and the window is offset by the same value as the horizon.


\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.7]{img/rollingwindow.png}
  \caption{Rolling window principle\cite{rolling}.}
  \label{fig:rolling}
\end{figure}


\subsubsection{Error measures and forecasting performance evaluation}\label{errors}

This subsection will contain a brief description of all the errors measures that I show in my tool. I offer a large panel of choice regarding errors so that anyone can analyse the one that best suits its needs. 

The errors shown in the tool are listed in the table below  : \\


\begin{tabular}{|l|c|r|}
  \hline
   Full Name & Acronym & Formula \\
  \hline
  Mean Squared Error & MSE & $\frac{1}{n}\sum_{t=0}^n (y_t - \hat{y}_t)^2$ \\
  Root Mean Squared Error & RMSE & $\sqrt{\frac{1}{n} \sum_{t=0}^n (y_t - \hat{y}_t)^2}$ \\
  Mean Absolute Error & MAE & $\frac{1}{n} \sum_{t=0}^n |y_t - \hat{y}_t|$ \\
  Mean Absolute Percentage Error & MAPE & $\frac{1}{n} \sum_{t=0}^n  \mid 100 \cdot \frac{y_t - \hat{y}_t}{y_t}\mid$ \\
  Scaled Mean Absolute Percentage Error & sMAPE & $\frac{100}{n} \sum_{t=0}^n \cdot \frac{\mid y_t - \hat{y}_t\mid}{\frac{y_t+\hat{y}_t}{2}}$ \\
  Mean Absolute Scaled Error & MASE & $\frac{1}{n}\sum_{t=1}^n \left( \frac{\left| y_t - \hat{y}_t \right|}{\frac{1}{n-1}\sum_{i=2}^n \left| y_t-y_{t-1} \right|}\right)$ \\
  Normalized Mean Squared Error & NMSE & $\frac{1}{n}\frac{\sum_{t=0}^{n} (y_t-\hat{y}_t)^2}{var(y_t)}$ \\
  Normalized Naive Mean Squared Error & NNMSE & $\frac{1}{n}\frac{\sum_{t=0}^{n} (y_t-\hat{y}_t)^2}{\sum_{t=0}^{n} (y_t-\hat{y}_{Naive,t})^2}$ \\
  \hline
\end{tabular}
\\



Remarkable notes : 

\begin{itemize}
    \item MAPE is generally used for reporting to outsiders, because it expresses an error in percentages that anyone can imagine and understand.
    \item MASE is often the best performing performance meter because it avoids most of the problems induced by other error measures such as : scale independence, symmetry,... 
    \item NNMSE is a relative error showing how a model is better performing than the naive method. 
\end{itemize}



\subsection{Model selection}

Finally, model selection is the problem of choosing which method to use for a pool of possible methods. The best model should be chosen as the final model with the best related parametrisations. What is described as the best model is the model that performs the best on the validation set (i.e.\ testing set). That model should have the strongest capability of generalising, a good balance between fitting rightness and simplicity. \cite{BenTaieb}





\chapter{Forecasting tool}\label{tool}

As a major contribution for my master's thesis, I developed a web application tool in \textbf{$R$} that offers an easy and fast way to compare different machine learning models, forecasting some financial time series. The tool runs online and allows anyone to choose a financial time series, modify it, apply some machine learning models on it and tuning their parameters. 

The rest of this chapter includes a tutorial on how to use the tool, information on the data sources and the algorithms used in this tool and a section on how to include more models and data sources. 


\section{Presentation of the tool}

This section describes how the tool works and what its components are. The web page is divided in three tabs : one for the data selection and visualisation, the second one to try and compare forecasting models and the last tab is for errors analysis. 

Let's have a look at the first tab on figure \ref{fig:tool1} and its components descriptions.

\subsection{Tab : Data inspection}


\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.44]{img/tab1.png}
  \caption{View of the first tab of the tool.}
  \label{fig:tool1}
\end{figure}


\begin{enumerate}
\item This option allows one to choose a market. The corresponding data will be downloaded from $Yahoo!$ $Finance$. The data is automatically refreshed when needed.
\item For each market there are different selectable options such as : the opening, closing, high and low prices and the volume.
\item Rather than using the raw data, a transformation can be selected among the volatility of the data and the compound returns.
\item The time series can be cut in time as desired by adjusting the starting date on the left and the end date on the right. This way, one can position his time series himself wherever he wants back in time.
\item A plot to visualise the data as chosen by all the parameters.
\item A summary of the data showing basic informations like the mean, first and third quartiles,...
\end{enumerate}


\subsection{Tab : Multi-step ahead forecasting}

\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.44]{img/tab2.png}
  \caption{View of the second tab of the tool.}
  \label{fig:tool2}
\end{figure}

\begin{enumerate}
\item\inlineitem Those are the configurable parameters of the SVM : C, the cost \ref{cost} and $\gamma$, the Gaussian RBF kernel parameter. The Knn algorithm already finds the best parameter for $K$ with a fast automated cross-validation.
\item The order of the model, i.e.\ the number of days which are used to predict a value $Y$ at time $t$.
\item The horizon is the number of days we try to predict in multi-step ahead forecasting.
\item The strategies are defined in detail in section \ref{strat}.
\item Those are some options to enable or disable the plotting of the models for a clearer visibility.
\item This is the resulting plot with its legend. The plot is made with the library \textbf{plotly} that gives a lot of options like zooming, downloading,... The orange dotted line shows the separation between the training set and the test set. Here, we are manually toying with parameters and therefore the line represents the horizon cut, which may be different during model validation.
\item This is a short table which resumes the errors of the models, as explained in section \ref{errors}, for the current forecasting. For a more general idea of how a model is usually performing, one should refer to the third and last tab of the web page. 
\end{enumerate}



\subsection{Tab : Average error}

\begin{figure}[!h]
  \centering
    \includegraphics[scale=0.44]{img/tab3.png}
  \caption{View of the third tab of the tool.}
  \label{fig:tool3}
\end{figure}


\begin{enumerate}
\item\inlineitem\inlineitem\inlineitem\inlineitem\inlineitem Those are the same parameters as those in the second tab.
\item This is a table of means of errors. Using the selected parameters, each model is applied through the whole dataset with a rolling window. The errors are averaged for each window over the whole dataset by applying the same model.
\end{enumerate}




\section{Datasets}

The tool proposes some default datasets such as data from $CAC40$, $S\&P 500$,... These datasets are directly downloaded from $Yahoo!$ $Finance$ and are daily refreshed. The data is publicly available and entirely free.

Each dataset is composed of : dates, opening, high, low and closing prices (OHLC) and the volumes of the stock for each weekday. 


\section{Always up-to-date}

The tool ensures that the data is automatically refreshed when new data is available. When new data is downloaded, the models are also automatically updated with the new data without having to do anything. This is an important feature of the tool because it really shows its independence. The tool acts like a standalone app that constantly provides up-to-date information. 


\section{Libraries}

The tool is written in \textbf{$R$} \cite{R} and it's web client-server part is mostly handled by a library called \textbf{Shiny} \cite{Shiny}.

\textbf{Shiny} is a library that proposes a web application framework that handles everything, including client-server, on itself. It allows one to only care about the functionalities of the code and the library will take care of how things are updated on the client part, how the client interacts with the server, ...

As for the rest, I use \textbf{Plotly} \cite{plotly} for the nice visual appearance plots and their nice interaction with the user. I use \textbf{e1071} \cite{e1071} for the SVM's models and \textbf{gbcode} \cite{gbcode} for the KNNs models.

Finally, the \textbf{quantmod} \cite{quantmod} package is a quantitative financial modelling framework and can be used for modelling in finance. The function $getSymbols$ of this library is used to download the datasets from the Internet.


\section{Code}

Due to the \textbf{Shiny} library, the code of the tool is very loosely coupled and easy to read. The code is decomposed in one file for each of the client-server parts, one configuration file, one for the errors measurements and one file per machine learning algorithm. 

There is no apparent links between the server and the client because \textbf{Shiny} handles it all. The client part contains a list of widgets to display accompanied by their positioning and a unique tag that refers to a variable from the server that they will be linked to. On the other side, the server contains all the logic of the program and stores its results in variables that the client will access thanks to their tag/variable name.

The configuration file, for its part, contains a list of datasets twinned with their respectively tag reference on $Yahoo!$ $Finance$. 

The rest of this section contains more information on how one could tweak the code and integrate new datasets and models.


\subsection{Add new datasets}

To add a new dataset, one has to find the tag reference of the market on $Yahoo!$ $Finance$ and add it to the configuration file following the same pattern as those already added; see the content of the configration file below : 


\begin{lstlisting}
markets        <- c( "^FCHI",  "^IXIC",   "^GSPC")
names(markets) <- c("CAC 40", "NASDAQ", "S&P 500")
\end{lstlisting}

where the $markets$ are the tags, and the $names(markets)$ are the names displayed in the application.


\subsection{Add new models}

To add a new model, one can make a new function that takes in input a dataframe, a horizon and more parameters, and that returns the final predictions as a list of values. As an example, below is the header of the function that computes the predictions with SVM : \\


\begin{lstlisting}
svmForecast <- function(df, horizon, window, gamma = 1/window, 
                            cost = 1, strategy = "recursive"){
    ...
    return predictedY
}
\end{lstlisting}


On the client part, nothing has to be changed. Instead, there must be some inclusions of the new model computations on the server part.

The server has then to be updated by adding the new model in the following functions : \textit{predPlot}, \textit{predTable}, \textit{errorTable}. They respectively correspond to the plotting function of the second tab, the predictions errors table on the second tab and the averaged errors of the rolling window table on the third tab of the tool. As an example, below is a piece of code of the function \textit{predPlot} : \\

\begin{lstlisting}
output$predPlot <- renderPlotly({
    # Initial plot
    p <- plot_ly(y = df$target, x = df$date, type = 'scatter',
                    mode = 'lines', name = 'Target')
    ...
    # Add forecasts for each model
    predictedY <- svmForecast(df, trainingcut(), window(), gamma(), 
                                    cost(), strategy())
    p <- p %>% add_trace(x = getTestDf()$date, y = predictedY, 
                                mode = 'lines')
    ...
}
\end{lstlisting}

First, the plot is created with \textbf{plotly}, and then the traces of each model's predictions are added to the plot. 

Another example is the function \textit{predTable} that shows the errors table on the second tab of the tool : \\

\begin{lstlisting}
output$predTable <- renderTable(rownames = TRUE, bordered = TRUE, {
    df <- getDf()
    ...
    
    # Get predictions for each model
    svmPredY <- svmForecast(df, trainingcut(), modelOrder(), gamma(), cost(), strategy())
    defaultSvmPredY <- svmForecast(df, trainingcut(), modelOrder(), strategy = strategy())
    knnPredY <- modelSelectionKNN(df$target, trainingcut(), modelOrder(), strategy())$forecasts
    naivePredY <- naiveForecast(df, trainingcut())

    ...
    
    # Show the means of all errors in table
    as.data.frame(getErrors(svmPredY, defaultSvmPredY, knnPredY, naivePredY, getTestDf()$target))
  })
\end{lstlisting}  
  
As one can see, the code is just about forecasting values with each model and compute the errors with the actual values. Adding a new model here is about adding a call to the model and adding the predictions to the errors computations. The function \textit{errorTable} is nearly the same. Note: the $getErrors()$ function doesn't exists but it represents the actual computations of the table that in reality takes too much place as the table is very big. 

A special note : the \textbf{Shiny} library handles the fact that if a same function is called for multiple outputs, it is only computed once and the same results are returned for each output. Therefore, we can call \textit{svmForecast} for the plotting and the errors computations without doing over-computations because internally, \textbf{Shiny} handles it all and will update the results automatically if any of the parameters is changed.

So any new model can be included in those functions by following the same structure as the models already present. Nothing else has to be changed.


\chapter{Results}\label{results}

In this chapter, one will find the descriptions of the experiments I conducted in order to find the best parameters for each model, i.e.\ the parameters that lead to the smallest errors. After analysing each algorithm on its own, one will find a section comparing and discussing the results of the algorithms between them.


\section{Description of my experiments}

I conducted experiments to search for the best parameters of each algorithm to perform forecasting with a search grid. I will compare the averaged MSE and MASE over a rolling window validation as explained in \ref{rwindow}. First I will run the experiments on a reduced history of 6-12 months in order to select the most efficient model. After that, I will use the optimal models on a longer history (e.g. 10-15 years) to evaluate their performance over the long term and see if there is a model that is consistently performing better than the others. 

The experiments will be run on the volatility of the \textit{CAC40} times series. I will use two different proxies for the volatility : the proxy described by equation \ref{volatility} also used in the tool (proxy 1), and the rolling standard deviation with a window of 21 days (proxy 2). It will be interesting to see if the smoothness of proxy 2 has any positive effects for forecasting, or if daily volatility has more benefits. All the experiments are run with a window of 21 days and a horizon of 7 days which are values that I found to perform well while using my tool.

I made a preliminary analysis by testing the SVM parameters with the tool I developed in order to better lead my grid searches. Therefore, not all results will be displayed with the same granularity.

Concerning the search grid process of SVM parameters, searching through exponentially growing sequences of $C$ and $\gamma$, such as powers of 2, is a practical method to identify good parameters. \cite{Hsu}


\subsection{Reduced history}

I first ran the experiment on a 9 months history. One will find on figure \ref{fig:plot6m} the plots of the time series with the applied proxies.


\begin{figure}[!h]
\centering
\makebox[\linewidth]{%
\begin{subfigure}{.55\linewidth}
  \centering
  \includegraphics[width=\linewidth]{img/plot6sigma.png}
  \caption{Using proxy 1.}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.55\linewidth}
  \centering
  \includegraphics[width=\linewidth]{img/plot6ma.png}
  \caption{Using proxy 2.}
  \label{fig:sub2}
\end{subfigure}}
\caption{Plots of the reduced time series using the two proxies.}
\label{fig:plot6m}
\end{figure}


The results of the search grid for the SVM parameters can be visualised on the heat maps on figure \ref{fig:heat6m} where the resulting errors are displayed according to the parameters. The complete numeric results of the search grid can be found in appendix \ref{6mapp}. 

As on can see on the heat map, the most optimal cost and gamma parameters seem to lie on a diagonal. This is an indication of how one should use those parameters. The heat maps for the proxy 1 required a special scaling because the values are much smaller than with the proxy 2 as one could see on figure \ref{fig:plot6m}.

Then on figures \ref{fig:table6m}, \ref{fig:table6mKnn} and \ref{fig:table6mNaive} in the next subsections, one will find, for each proxy, the results of the experiments for SVM, KNN and the naive method. First, the results will show a table of the parameters that lead to the smallest related error measure, which is the averaged error over the whole rolling window validation process. Below the table, one will find box-plots of all the errors for the best parameters. With the help of the box-plots, one can better imagine the relevancy of the averaged errors. Indeed, in most of the box-plots, we can observe very large outliers absorbing a large part of the mean. As an example, one can see, for the MSE of the SVM results using proxy 2, that the complete box seem to lay under the averaged error because of outliers that are 10 times bigger than normal. By observation on the plots of the time series on figure \ref{fig:plot6m}, we could image that those extreme outliers could be due to the very large peaks of volatility that both proxies show.


\input{heatmapsreduced.tex}

\clearpage
\subsubsection{SVM results}
\input{resultsreduced.tex}

\clearpage
\subsubsection{KNN results}
\input{resultsreducedKnn.tex}

\clearpage
\subsubsection{Naive results}
\input{resultsreducedNaive.tex}



\clearpage
\subsection{Longer history}

Then, I ran the experiment on a 10 years history. One will find on figure \ref{fig:plot10y} the plots of the time series with the applied proxies.


\begin{figure}[!h]
\centering
\makebox[\linewidth]{%
\begin{subfigure}{.6\linewidth}
  \centering
  \includegraphics[width=\linewidth]{img/plot10sigma.png}
  \caption{Using proxy 1.}
  \label{fig:sub2.1}
\end{subfigure}%
\begin{subfigure}{.6\linewidth}
  \centering
  \includegraphics[width=\linewidth]{img/plot10ma.png}
  \caption{Using proxy 2.}
  \label{fig:sub2.2}
\end{subfigure}}
\caption{Plots of the longer time series using the two proxies.}
\label{fig:plot10y}
\end{figure}



Then on figure \ref{fig:table10y}, one will find, for each proxy, the results of the experiments expressed as the cost and gamma that lead to the smallest related error measure.


The results of the search grid for the SVM parameters can be visualised on the heat maps on figure \ref{fig:heat10y} where the resulting errors are displayed according to the parameters. The complete numeric results of the search grid can be found in appendix \ref{10yapp}. 

In the same way as the reduced history heat maps, on can see on the heat map that the most optimal cost and gamma parameters seem to lie on a diagonal.

On figures \ref{fig:table10y}, \ref{fig:table10yKnn} and \ref{fig:table10yNaive} in the next subsections, one will find, for each proxy, the results of the experiments for SVM, KNN and the naive method. The results follow the same structure as those of the reduced history.



\input{heatmapslonger.tex}


\clearpage
\subsubsection{SVM results}
\input{resultslonger.tex}

\clearpage
\subsubsection{KNN results}
\input{resultslongerKnn.tex}

\clearpage
\subsubsection{Naive results}
\input{resultslongerNaive.tex}



\clearpage
\subsection{Comparison of results}


The first observation that I should point out is that, looking on figures \ref{fig:compare6m} and \ref{fig:compare10y}, regarding the MASE error measure, the proxy 1 is systematically providing much better results and it's not even close. Regarding MSE, we can't make any deductions between the proxies. 

Another observation comes from the fact that we already observed that for SVM, the best parameters lay on a diagonal. The results of the reduced history for proxy 2 are related to the results of the longer history by this rule. However this is not the case for proxy 1, where results seem to lay on a parallel diagonal, meaning that we couldn't extrapolate short term observations for making long term decisions.

For KNN, the values of $K$ are more or less the same for short and long histories for a same proxy. The interesting observation here is that the values of $K$ when using the proxy 2 are very low. It seems that the smoothness of the time series, due to its moving window standard deviation, make it that to predict a value, the best options is to look at the very last points, the very nearest neighbors.


\input{reducedhistorycomp.tex}

\input{longhistorycomp.tex}


Now that we have all the results for the optimal parameters of the models, it's time to compare the models between them. For that, we should look at the results of the longer history, on figure \ref{fig:compare10y}, which is more representative. 

With proxy 1, SVM is the best model by far regarding MSE, but it lays slightly behind KNN regarding MASE. 

With proxy 2, regarding MSE, KNN looks like it is performing quite better than SVM ($\sim 16\%$ better) but if we look at the box-plots of those results, we can see that the median of SVM is below 100 with outliers above scores of 2500, so in reality the results are pretty close and might be slightly favourable to SVM. On the other hand, SVM is closely ahead of KNN regarding MASE.

All in all, the SVM and KNN models outperformed the naive model at any point.




\chapter{Conclusion \& Future Work }


\section{Conclusion}


\section{Future work}




\clearpage
\appendix
\chapter{Search grid on reduced history}\label{6mapp}

\input{appendix/proxy1MSE6M.tex}

\input{appendix/proxy1MASE6M.tex}

\input{appendix/proxy2MSE6M.tex}

\input{appendix/proxy2MASE6M.tex}


\chapter{Search grid on longer history}\label{10yapp}

\input{appendix/proxy1MSE10y.tex}

\input{appendix/proxy1MASE10y.tex}

\input{appendix/proxy2MSE10y.tex}

\input{appendix/proxy2MASE10y.tex}



\bibliographystyle{unsrt}
\bibliography{biblio.bib}


\end{document}